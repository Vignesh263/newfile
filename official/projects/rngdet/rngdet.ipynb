{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# RNGDet evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2FlaQcEPOER"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install and import the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-08 08:17:38.281659: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-09-08 08:17:38.317879: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:8942] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-09-08 08:17:38.317907: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-09-08 08:17:38.317926: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-09-08 08:17:38.324551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-08 08:17:39.076772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ghpark/.conda/envs/nightly/lib/python3.9/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /home/ghpark/.conda/envs/nightly/lib/python3.9/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NHT1iiIiBzlC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ghpark/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-09-08 08:17:44.337926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1884] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22275 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:60:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "from official.projects.rngdet.tasks import rngdet\n",
        "from official.core import exp_factory\n",
        "exp_config = exp_factory.get_exp_config('rngdet_cityscale')\n",
        "task_obj = rngdet.RNGDetTask(exp_config.task)\n",
        "model = task_obj.build_model()\n",
        "#task_obj.initialize(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.backbone.get_weights()[0][0,0,0])\n",
        "print(\"------------------\")\n",
        "print(\"------------------\")\n",
        "print(model._query_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ghpark/02_RNGDet/ckpt/test_06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f3c30376790>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ckpt_dir_or_file = '/home/ghpark/02_RNGDet/ckpt/test_06'\n",
        "#if tf.io.gfile.isdir(ckpt_dir_or_file):\n",
        "#  ckpt_dir_or_file = tf.train.latest_checkpoint(ckpt_dir_or_file)\n",
        "print(ckpt_dir_or_file)\n",
        "#ckpt = tf.train.Checkpoint(backbone=model.backbone, backbone_history=model.backbone_history)\n",
        "#ckpt = tf.train.Checkpoint(backbone=model.backbone)\n",
        "ckpt = tf.train.Checkpoint(\n",
        "    backbone=model.backbone,\n",
        "    backbone_history=model.backbone_history,\n",
        "    transformer=model.transformer,\n",
        "    query_embeddings=model._query_embeddings,\n",
        "    segment_head=model._segment_head,\n",
        "    keypoint_head=model._keypoint_head,\n",
        "    class_embed=model._class_embed)\n",
        "status = ckpt.restore(tf.train.latest_checkpoint(ckpt_dir_or_file))\n",
        "status.expect_partial().assert_existing_objects_matched()\n",
        "#status.assert_consumed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.backbone.get_weights()[0][0,0,0])\n",
        "print(\"------------------\")\n",
        "print(\"------------------\")\n",
        "print(model._query_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-08 08:17:55.014429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:440] Loaded cuDNN version 8903\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1084 initial candidates extracted from the segmentation map...\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from official.projects.rngdet.eval import agent\n",
        "\n",
        "pad_size = 128\n",
        "sat_image = np.array(Image.open(os.path.join('./region_0_sat.png')))\n",
        "#sat_image = np.pad(sat_image,np.array(((pad_size,pad_size),(pad_size,pad_size),(0,0))),'constant')\n",
        "#Image.fromarray(sat_image.astype(np.uint8)).convert('RGB').save(f'./temp.png')\n",
        "\n",
        "sat_image = tf.cast(sat_image, tf.float32)\n",
        "agent = agent.Agent(model, sat_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 3: Finsh exploration. Save visualization and graph...\n"
          ]
        }
      ],
      "source": [
        "logit_threshold = 0.75\n",
        "roi_size = 128\n",
        "\n",
        "while 1:\n",
        "    agent.step_counter += 1\n",
        "    # crop ROI\n",
        "    sat_ROI, historical_ROI = agent.crop_ROI(agent.current_coord)\n",
        "    sat_ROI = tf.expand_dims(sat_ROI, 0) / 255.0\n",
        "    # (gunho) historical_ROI / 255.0 in original code\n",
        "    historical_ROI = tf.expand_dims(historical_ROI, 0) / 255.0\n",
        "    historical_ROI = tf.expand_dims(historical_ROI, -1)\n",
        "    historical_ROI = tf.cast(historical_ROI, tf.float32)\n",
        "    # predict vertices in the next step\n",
        "    outputs, pred_segment, pred_keypoint = model(sat_ROI, historical_ROI, training=False)\n",
        "    # agent moves\n",
        "    # alignment vertices\n",
        "    outputs = outputs[-1]\n",
        "    pred_coords = outputs['box_outputs']\n",
        "    pred_probs = outputs['cls_outputs']\n",
        "    alignment_vertices = [[v[0]-agent.current_coord[0]+agent.crop_size//2,\n",
        "        v[1]-agent.current_coord[1]+agent.crop_size//2] for v in agent.historical_vertices]\n",
        "    pred_coords_ROI = agent.step(pred_probs,pred_coords,thr=logit_threshold)\n",
        "    \n",
        "    if agent.finish_current_image:\n",
        "        print(f'STEP 3: Finsh exploration. Save visualization and graph...')\n",
        "        Image.fromarray(\n",
        "            agent.historical_map[roi_size:-roi_size,roi_size:-roi_size].astype(np.uint8)\n",
        "            ).convert('RGB').save(f'./segmentation/0_result.png')\n",
        "        break\n",
        "    # stop action\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Vertex():\n",
        "    def __init__(self,v,id):\n",
        "        self.x = v[0]\n",
        "        self.y = v[1]\n",
        "        self.id = id\n",
        "        self.neighbors = []\n",
        "\n",
        "class Edge():\n",
        "    def __init__(self,src,dst,id):\n",
        "        self.src = src\n",
        "        self.dst = dst\n",
        "        self.id = id\n",
        "\n",
        "class Graph():\n",
        "    def __init__(self):\n",
        "        self.vertices = {}\n",
        "        self.edges = {}\n",
        "        self.vertex_num = 0\n",
        "        self.edge_num = 0\n",
        "\n",
        "    def find_v(self,v_coord):\n",
        "        if f'{v_coord[0]}_{v_coord[1]}' in self.vertices.keys():\n",
        "            return self.vertices[f'{v_coord[0]}_{v_coord[1]}']\n",
        "        return \n",
        "\n",
        "    def find_e(self,v1,v2):\n",
        "        if f'{v1.id}_{v2.id}' in self.edges:\n",
        "            return True\n",
        "        return None\n",
        "\n",
        "    def add(self,edge):\n",
        "        v1_coord = edge[0]\n",
        "        v2_coord = edge[1]\n",
        "        v1 = self.find_v(v1_coord)\n",
        "        if v1 is None:\n",
        "            v1 = Vertex(v1_coord,self.vertex_num)\n",
        "            self.vertex_num += 1\n",
        "            self.vertices[f'{v1.x}_{v1.y}'] = v1\n",
        "        \n",
        "        v2 = self.find_v(v2_coord)\n",
        "        if v2 is None:\n",
        "            v2 = Vertex(v2_coord,self.vertex_num)\n",
        "            self.vertex_num += 1\n",
        "            self.vertices[f'{v2.x}_{v2.y}'] = v2\n",
        "\n",
        "        if v1 not in v2.neighbors:\n",
        "            v2.neighbors.append(v1)\n",
        "        if v2 not in v1.neighbors:\n",
        "            v1.neighbors.append(v2)\n",
        "        e = self.find_e(v1,v2)\n",
        "        if e is None:\n",
        "            self.edges[f'{v1.id}_{v2.id}'] = Edge(v1,v2,self.edge_num)\n",
        "            self.edge_num += 1\n",
        "            self.edges[f'{v2.id}_{v1.id}'] = Edge(v2,v1,self.edge_num)\n",
        "            self.edge_num += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "# save generated graph\n",
        "graph = Graph()\n",
        "try:\n",
        "    with open(f'./segmentation/0.json','w') as jf:\n",
        "        json.dump(agent.historical_edges,jf)\n",
        "except Exception as e:\n",
        "    print('Error...')\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a8582ebde7b"
      },
      "source": [
        "Use `ds_info` (which is an instance of `tfds.core.DatasetInfo`) to lookup the text descriptions of each class ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDys5bZ1zsml"
      },
      "source": [
        "Run a batch of the processed training data through the model, and view the results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "classification_with_model_garden.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
