{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O_9R0Cnd8X_y",
    "outputId": "33c3d2c0-e6e8-44f8-a4b2-abe5af587c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import random\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8014Lyb8X_1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Input\n",
    "from tensorflow.keras.layers import add, AveragePooling2D, Dense, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow_addons.optimizers import SGDW\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6c0f-mYu8X_3"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WIFinrN8X_5"
   },
   "outputs": [],
   "source": [
    "def augment(img, label):\n",
    "    image = tf.cast(img, tf.float32)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.resize_with_pad(image, 36, 36)\n",
    "    image = tf.image.random_crop(image, size=[32, 32, 3])\n",
    "    image = (image / 255.0)\n",
    "    return image, label\n",
    "\n",
    "def normalize(img, label):\n",
    "    image = tf.cast(img, tf.float32)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.resize_with_pad(image, 36, 36)\n",
    "    image = tf.image.random_crop(image, size=[32, 32, 3])\n",
    "    image = (image / 255.0)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = X_train[5000:], X_train[:5000]\n",
    "y_train, y_val = y_train[5000:], y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-U7-JXp78X_7"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.cache().shuffle(2048).map(augment, AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(128).prefetch(AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.cache().shuffle(2048).map(augment, AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(128).prefetch(AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.cache().shuffle(2048).map(augment, AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(128).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uio5Tasr8X__"
   },
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    @staticmethod\n",
    "    def residual_block(data, filters, strides, transition):\n",
    "        shortcut = data\n",
    "\n",
    "        x = Conv2D(filters, 3, strides, padding=\"same\", kernel_initializer=\"he_normal\", \n",
    "                   use_bias=False)(data)\n",
    "        x = BatchNormalization(beta_initializer='zeros', gamma_initializer=RandomNormal(mean=0.0, stddev=1.0))(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(filters, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\", \n",
    "                   use_bias=False)(x)\n",
    "        x = BatchNormalization(beta_initializer='zeros', gamma_initializer=RandomNormal(mean=0.0, stddev=1.0))(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        if transition:\n",
    "            shortcut = Conv2D(filters, 1, 2, padding=\"valid\", kernel_initializer=\"he_normal\", \n",
    "                              use_bias=False)(shortcut)\n",
    "            shortcut = BatchNormalization(beta_initializer='zeros', \n",
    "                                          gamma_initializer=RandomNormal(mean=0.0, stddev=1.0))(shortcut)\n",
    "\n",
    "        return add([shortcut, x])\n",
    "    \n",
    "    @staticmethod\n",
    "    def build(num_blocks=2, filters_block=[16,32,64]):\n",
    "        inputs = Input(shape=(32,32,3))\n",
    "        x = Conv2D(16, 3, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(num_blocks):\n",
    "                if j==0:\n",
    "                    transition = True\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    transition = False\n",
    "                    strides = 1\n",
    "                    \n",
    "                x = ResNet.residual_block(x, filters_block[i], strides, transition)\n",
    "                \n",
    "        avg_pool = AveragePooling2D(3)(x)\n",
    "        x = Dense(10, use_bias=False, kernel_initializer='he_normal')(avg_pool)\n",
    "        x = Flatten()(x)        \n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "                \n",
    "        return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwjm6vPG8YAB"
   },
   "outputs": [],
   "source": [
    "model = ResNet.build(num_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ovmPAmM48YAD",
    "outputId": "c5fbaf3b-325a-4401-e3ec-0480c45fbd64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Conv layers: 14\n"
     ]
    }
   ],
   "source": [
    "count_conv = 0\n",
    "for layer in model.layers:\n",
    "    if not isinstance(layer, BatchNormalization):\n",
    "        if hasattr(layer, 'trainable'):\n",
    "            layer.trainable = False\n",
    "    if isinstance(layer, Conv2D):\n",
    "        count_conv += 1\n",
    "print(f'Total Number of Conv layers: {count_conv - 2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jGyMNrkM8YAF",
    "outputId": "d09159f8-e518-4bc2-af75-925ff6729a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 32, 32, 16)   432         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 16)   2304        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 16)   64          conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 16, 16, 16)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 16)   2304        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 16)   256         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 16)   64          conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 16)   64          conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 16, 16, 16)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 16, 16, 16)   0           batch_normalization_62[0][0]     \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 16)   2304        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 16)   64          conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 16, 16)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 16, 16, 16)   2304        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 16)   64          conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 16)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 16, 16, 16)   0           add_24[0][0]                     \n",
      "                                                                 activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 8, 8, 32)     4608        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 8, 32)     128         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 8, 8, 32)     0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 32)     9216        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 32)     512         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 8, 32)     128         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 8, 32)     128         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 8, 8, 32)     0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 8, 8, 32)     0           batch_normalization_67[0][0]     \n",
      "                                                                 activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 32)     9216        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 8, 32)     128         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 8, 8, 32)     0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 8, 32)     9216        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 8, 32)     128         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 8, 8, 32)     0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 8, 8, 32)     0           add_26[0][0]                     \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 4, 4, 64)     18432       add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 4, 4, 64)     256         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 4, 4, 64)     0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 4, 4, 64)     36864       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 4, 4, 64)     2048        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 4, 4, 64)     256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 4, 4, 64)     256         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 4, 4, 64)     0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 4, 4, 64)     0           batch_normalization_72[0][0]     \n",
      "                                                                 activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 4, 4, 64)     36864       add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 4, 4, 64)     256         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 4, 4, 64)     0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 4, 4, 64)     36864       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 4, 4, 64)     256         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 4, 4, 64)     0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 4, 4, 64)     0           add_28[0][0]                     \n",
      "                                                                 activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 1, 1, 64)     0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 1, 10)     640         average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 10)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 10)           0           flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 176,624\n",
      "Trainable params: 1,120\n",
      "Non-trainable params: 175,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TEPlznc8YAI"
   },
   "outputs": [],
   "source": [
    "step = tf.Variable(0, trainable=False)\n",
    "schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    [28125, 42185], [1e-0, 1e-1, 1e-2])\n",
    "\n",
    "wd = lambda: 1e-4 * schedule(step)\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.1, momentum=0.9), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cF19vf5r8YAJ"
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch in [80, 120]:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "    \n",
    "callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "C_JPlCWT8YAL",
    "outputId": "31510a69-a8a6-46d4-9607-e3d563329290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 1/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.0223 - accuracy: 0.2524 - val_loss: 1.9104 - val_accuracy: 0.2928\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 2/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.8265 - accuracy: 0.3335 - val_loss: 1.7939 - val_accuracy: 0.3414\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 3/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.7658 - accuracy: 0.3585 - val_loss: 1.7587 - val_accuracy: 0.3644\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 4/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.7321 - accuracy: 0.3673 - val_loss: 1.7299 - val_accuracy: 0.3680\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 5/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.7036 - accuracy: 0.3765 - val_loss: 1.7043 - val_accuracy: 0.3786\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 6/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.6777 - accuracy: 0.3846 - val_loss: 1.6711 - val_accuracy: 0.3900\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 7/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6569 - accuracy: 0.3944 - val_loss: 1.6565 - val_accuracy: 0.3966\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 8/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6462 - accuracy: 0.3975 - val_loss: 1.6269 - val_accuracy: 0.3944\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 9/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6321 - accuracy: 0.4004 - val_loss: 1.6347 - val_accuracy: 0.3968\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 10/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6253 - accuracy: 0.4053 - val_loss: 1.6323 - val_accuracy: 0.3926\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 11/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6168 - accuracy: 0.4081 - val_loss: 1.6002 - val_accuracy: 0.4134\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 12/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6082 - accuracy: 0.4118 - val_loss: 1.6326 - val_accuracy: 0.4062\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 13/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6057 - accuracy: 0.4159 - val_loss: 1.5931 - val_accuracy: 0.4136\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 14/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5974 - accuracy: 0.4158 - val_loss: 1.5904 - val_accuracy: 0.4202\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 15/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5971 - accuracy: 0.4157 - val_loss: 1.5963 - val_accuracy: 0.4160\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 16/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5875 - accuracy: 0.4214 - val_loss: 1.5920 - val_accuracy: 0.4136\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 17/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5897 - accuracy: 0.4224 - val_loss: 1.5882 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 18/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5806 - accuracy: 0.4262 - val_loss: 1.5791 - val_accuracy: 0.4206\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 19/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5795 - accuracy: 0.4280 - val_loss: 1.5862 - val_accuracy: 0.4116\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 20/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5773 - accuracy: 0.4245 - val_loss: 1.5779 - val_accuracy: 0.4256\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 21/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5789 - accuracy: 0.4245 - val_loss: 1.6044 - val_accuracy: 0.4172\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 22/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5728 - accuracy: 0.4263 - val_loss: 1.5836 - val_accuracy: 0.4182\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 23/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5733 - accuracy: 0.4277 - val_loss: 1.5667 - val_accuracy: 0.4310\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 24/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5649 - accuracy: 0.4294 - val_loss: 1.5904 - val_accuracy: 0.4218\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 25/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5667 - accuracy: 0.4282 - val_loss: 1.5658 - val_accuracy: 0.4326\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 26/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5648 - accuracy: 0.4306 - val_loss: 1.5579 - val_accuracy: 0.4278\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 27/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5606 - accuracy: 0.4320 - val_loss: 1.5763 - val_accuracy: 0.4218\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 28/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5659 - accuracy: 0.4293 - val_loss: 1.5790 - val_accuracy: 0.4228\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 29/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5580 - accuracy: 0.4324 - val_loss: 1.5520 - val_accuracy: 0.4316\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 30/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5573 - accuracy: 0.4333 - val_loss: 1.5566 - val_accuracy: 0.4304\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 31/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5607 - accuracy: 0.4311 - val_loss: 1.5730 - val_accuracy: 0.4252\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 32/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5580 - accuracy: 0.4307 - val_loss: 1.5744 - val_accuracy: 0.4310\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 33/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5549 - accuracy: 0.4349 - val_loss: 1.5594 - val_accuracy: 0.4296\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 34/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5527 - accuracy: 0.4335 - val_loss: 1.5592 - val_accuracy: 0.4288\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 35/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5515 - accuracy: 0.4365 - val_loss: 1.6294 - val_accuracy: 0.4176\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 36/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5490 - accuracy: 0.4365 - val_loss: 1.5572 - val_accuracy: 0.4338\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 37/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5517 - accuracy: 0.4339 - val_loss: 1.5534 - val_accuracy: 0.4338\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 38/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5478 - accuracy: 0.4347 - val_loss: 1.5422 - val_accuracy: 0.4362\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 39/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5457 - accuracy: 0.4341 - val_loss: 1.5292 - val_accuracy: 0.4408\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 40/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5441 - accuracy: 0.4378 - val_loss: 1.5329 - val_accuracy: 0.4434\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 41/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5447 - accuracy: 0.4371 - val_loss: 1.6048 - val_accuracy: 0.4178\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 42/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5413 - accuracy: 0.4387 - val_loss: 1.5960 - val_accuracy: 0.4330\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 43/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5408 - accuracy: 0.4385 - val_loss: 1.5462 - val_accuracy: 0.4424\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 44/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5359 - accuracy: 0.4386 - val_loss: 1.5672 - val_accuracy: 0.4352\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 45/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5412 - accuracy: 0.4414 - val_loss: 1.5454 - val_accuracy: 0.4386\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 46/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5400 - accuracy: 0.4388 - val_loss: 1.5660 - val_accuracy: 0.4242\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 47/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5365 - accuracy: 0.4391 - val_loss: 1.5424 - val_accuracy: 0.4376\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 48/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5363 - accuracy: 0.4393 - val_loss: 1.5364 - val_accuracy: 0.4436\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 49/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5381 - accuracy: 0.4383 - val_loss: 1.5645 - val_accuracy: 0.4278\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 50/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5402 - accuracy: 0.4385 - val_loss: 1.5534 - val_accuracy: 0.4302\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 51/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5331 - accuracy: 0.4446 - val_loss: 1.5336 - val_accuracy: 0.4428\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 52/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5352 - accuracy: 0.4401 - val_loss: 1.5486 - val_accuracy: 0.4364\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 53/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5301 - accuracy: 0.4434 - val_loss: 1.5573 - val_accuracy: 0.4410\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 54/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5298 - accuracy: 0.4461 - val_loss: 1.5462 - val_accuracy: 0.4446\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 55/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5311 - accuracy: 0.4446 - val_loss: 1.5414 - val_accuracy: 0.4438\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 56/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5274 - accuracy: 0.4451 - val_loss: 1.5686 - val_accuracy: 0.4346\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 57/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5311 - accuracy: 0.4426 - val_loss: 1.5511 - val_accuracy: 0.4364\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 58/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5309 - accuracy: 0.4421 - val_loss: 1.5135 - val_accuracy: 0.4566\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 59/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5258 - accuracy: 0.4444 - val_loss: 1.5224 - val_accuracy: 0.4436\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 60/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5259 - accuracy: 0.4467 - val_loss: 1.6437 - val_accuracy: 0.4172\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 61/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5305 - accuracy: 0.4428 - val_loss: 1.5214 - val_accuracy: 0.4484\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 62/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5295 - accuracy: 0.4430 - val_loss: 1.5290 - val_accuracy: 0.4450\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 63/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5246 - accuracy: 0.4452 - val_loss: 1.5157 - val_accuracy: 0.4482\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 64/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5315 - accuracy: 0.4413 - val_loss: 1.5157 - val_accuracy: 0.4470\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 65/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5261 - accuracy: 0.4438 - val_loss: 1.5499 - val_accuracy: 0.4376\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 66/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5272 - accuracy: 0.4459 - val_loss: 1.5326 - val_accuracy: 0.4414\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 67/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5296 - accuracy: 0.4437 - val_loss: 1.5378 - val_accuracy: 0.4312\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 68/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5234 - accuracy: 0.4439 - val_loss: 1.5410 - val_accuracy: 0.4406\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 69/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5219 - accuracy: 0.4458 - val_loss: 1.5483 - val_accuracy: 0.4372\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 70/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5259 - accuracy: 0.4453 - val_loss: 1.5364 - val_accuracy: 0.4404\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 71/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5262 - accuracy: 0.4457 - val_loss: 1.5360 - val_accuracy: 0.4376\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 72/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5233 - accuracy: 0.4485 - val_loss: 1.5332 - val_accuracy: 0.4404\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 73/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5225 - accuracy: 0.4453 - val_loss: 1.5171 - val_accuracy: 0.4548\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 74/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5243 - accuracy: 0.4470 - val_loss: 1.5128 - val_accuracy: 0.4544\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 75/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5257 - accuracy: 0.4450 - val_loss: 1.5070 - val_accuracy: 0.4532\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 76/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5196 - accuracy: 0.4462 - val_loss: 1.5220 - val_accuracy: 0.4408\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 77/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5236 - accuracy: 0.4448 - val_loss: 1.5313 - val_accuracy: 0.4414\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 78/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5181 - accuracy: 0.4474 - val_loss: 1.5019 - val_accuracy: 0.4542\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 79/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5234 - accuracy: 0.4435 - val_loss: 1.5671 - val_accuracy: 0.4420\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.10000000149011612.\n",
      "Epoch 80/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5210 - accuracy: 0.4462 - val_loss: 1.5323 - val_accuracy: 0.4414\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.010000000149011612.\n",
      "Epoch 81/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5106 - accuracy: 0.4507 - val_loss: 1.4792 - val_accuracy: 0.4562\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 82/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5059 - accuracy: 0.4537 - val_loss: 1.5013 - val_accuracy: 0.4528\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 83/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5038 - accuracy: 0.4550 - val_loss: 1.4986 - val_accuracy: 0.4594\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 84/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5056 - accuracy: 0.4538 - val_loss: 1.4815 - val_accuracy: 0.4622\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 85/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5033 - accuracy: 0.4535 - val_loss: 1.4905 - val_accuracy: 0.4590\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 86/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5056 - accuracy: 0.4528 - val_loss: 1.4918 - val_accuracy: 0.4560\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 87/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4999 - accuracy: 0.4569 - val_loss: 1.4895 - val_accuracy: 0.4534\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 88/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5010 - accuracy: 0.4569 - val_loss: 1.4808 - val_accuracy: 0.4584\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 89/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4989 - accuracy: 0.4558 - val_loss: 1.4832 - val_accuracy: 0.4612\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 90/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4996 - accuracy: 0.4573 - val_loss: 1.4924 - val_accuracy: 0.4568\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 91/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5015 - accuracy: 0.4555 - val_loss: 1.4942 - val_accuracy: 0.4584\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 92/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5038 - accuracy: 0.4528 - val_loss: 1.4900 - val_accuracy: 0.4526\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 93/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5022 - accuracy: 0.4548 - val_loss: 1.4875 - val_accuracy: 0.4594\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 94/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5004 - accuracy: 0.4566 - val_loss: 1.4952 - val_accuracy: 0.4584\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 95/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4992 - accuracy: 0.4583 - val_loss: 1.4887 - val_accuracy: 0.4662\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 96/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5001 - accuracy: 0.4545 - val_loss: 1.4977 - val_accuracy: 0.4530\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 97/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5003 - accuracy: 0.4559 - val_loss: 1.4851 - val_accuracy: 0.4682\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 98/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5031 - accuracy: 0.4555 - val_loss: 1.4871 - val_accuracy: 0.4588\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 99/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5023 - accuracy: 0.4559 - val_loss: 1.4821 - val_accuracy: 0.4576\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 100/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5014 - accuracy: 0.4539 - val_loss: 1.4925 - val_accuracy: 0.4534\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 101/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4972 - accuracy: 0.4596 - val_loss: 1.4930 - val_accuracy: 0.4602\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 102/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5021 - accuracy: 0.4576 - val_loss: 1.4951 - val_accuracy: 0.4532\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 103/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5047 - accuracy: 0.4534 - val_loss: 1.4852 - val_accuracy: 0.4544\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 104/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5001 - accuracy: 0.4564 - val_loss: 1.4939 - val_accuracy: 0.4550\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 105/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5005 - accuracy: 0.4590 - val_loss: 1.4869 - val_accuracy: 0.4580\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 106/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5018 - accuracy: 0.4546 - val_loss: 1.4935 - val_accuracy: 0.4588\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 107/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4965 - accuracy: 0.4584 - val_loss: 1.4960 - val_accuracy: 0.4552\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 108/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5007 - accuracy: 0.4577 - val_loss: 1.4917 - val_accuracy: 0.4564\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 109/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4987 - accuracy: 0.4581 - val_loss: 1.4864 - val_accuracy: 0.4590\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 110/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5005 - accuracy: 0.4539 - val_loss: 1.4899 - val_accuracy: 0.4502\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 111/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5037 - accuracy: 0.4536 - val_loss: 1.4815 - val_accuracy: 0.4602\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 112/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4980 - accuracy: 0.4575 - val_loss: 1.4906 - val_accuracy: 0.4570\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 113/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5009 - accuracy: 0.4533 - val_loss: 1.4784 - val_accuracy: 0.4688\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 114/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4997 - accuracy: 0.4548 - val_loss: 1.4860 - val_accuracy: 0.4628\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 115/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5014 - accuracy: 0.4551 - val_loss: 1.4816 - val_accuracy: 0.4556\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 116/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4956 - accuracy: 0.4577 - val_loss: 1.4818 - val_accuracy: 0.4612\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 117/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4993 - accuracy: 0.4566 - val_loss: 1.4793 - val_accuracy: 0.4626\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 118/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4965 - accuracy: 0.4580 - val_loss: 1.4851 - val_accuracy: 0.4560\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 119/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4983 - accuracy: 0.4559 - val_loss: 1.4804 - val_accuracy: 0.4604\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 120/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4979 - accuracy: 0.4567 - val_loss: 1.4948 - val_accuracy: 0.4598\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 121/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4952 - accuracy: 0.4569 - val_loss: 1.4855 - val_accuracy: 0.4630\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 122/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4958 - accuracy: 0.4557 - val_loss: 1.4891 - val_accuracy: 0.4574\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 123/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4986 - accuracy: 0.4581 - val_loss: 1.4801 - val_accuracy: 0.4546\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 124/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4999 - accuracy: 0.4569 - val_loss: 1.4849 - val_accuracy: 0.4672\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 125/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4961 - accuracy: 0.4579 - val_loss: 1.4853 - val_accuracy: 0.4564\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 126/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4975 - accuracy: 0.4580 - val_loss: 1.4819 - val_accuracy: 0.4610\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 127/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4999 - accuracy: 0.4554 - val_loss: 1.4842 - val_accuracy: 0.4580\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 128/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4968 - accuracy: 0.4561 - val_loss: 1.4841 - val_accuracy: 0.4620\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 129/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4973 - accuracy: 0.4584 - val_loss: 1.4840 - val_accuracy: 0.4590\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 130/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5008 - accuracy: 0.4575 - val_loss: 1.4846 - val_accuracy: 0.4582\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 131/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4965 - accuracy: 0.4580 - val_loss: 1.4862 - val_accuracy: 0.4642\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 132/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5003 - accuracy: 0.4569 - val_loss: 1.4847 - val_accuracy: 0.4686\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 133/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.4974 - accuracy: 0.4582 - val_loss: 1.4838 - val_accuracy: 0.4632\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 134/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4993 - accuracy: 0.4590 - val_loss: 1.4840 - val_accuracy: 0.4656\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 135/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5009 - accuracy: 0.4550 - val_loss: 1.4811 - val_accuracy: 0.4638\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 136/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.4974 - accuracy: 0.4590 - val_loss: 1.4824 - val_accuracy: 0.4660\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 137/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.4947 - accuracy: 0.4591 - val_loss: 1.4940 - val_accuracy: 0.4570\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 138/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4976 - accuracy: 0.4566 - val_loss: 1.4923 - val_accuracy: 0.4536\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 139/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4992 - accuracy: 0.4574 - val_loss: 1.4911 - val_accuracy: 0.4616\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 140/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5006 - accuracy: 0.4566 - val_loss: 1.4810 - val_accuracy: 0.4666\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 141/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.4962 - accuracy: 0.4590 - val_loss: 1.4814 - val_accuracy: 0.4584\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 142/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4960 - accuracy: 0.4583 - val_loss: 1.4846 - val_accuracy: 0.4588\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 143/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4974 - accuracy: 0.4583 - val_loss: 1.4730 - val_accuracy: 0.4646\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 144/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4984 - accuracy: 0.4560 - val_loss: 1.4892 - val_accuracy: 0.4540\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 145/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5002 - accuracy: 0.4556 - val_loss: 1.4862 - val_accuracy: 0.4588\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 146/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4961 - accuracy: 0.4579 - val_loss: 1.4793 - val_accuracy: 0.4578\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 147/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4924 - accuracy: 0.4591 - val_loss: 1.4886 - val_accuracy: 0.4588\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 148/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4993 - accuracy: 0.4563 - val_loss: 1.4793 - val_accuracy: 0.4624\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 149/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4975 - accuracy: 0.4571 - val_loss: 1.4936 - val_accuracy: 0.4588\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 150/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4926 - accuracy: 0.4589 - val_loss: 1.4742 - val_accuracy: 0.4556\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 151/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5012 - accuracy: 0.4564 - val_loss: 1.4912 - val_accuracy: 0.4566\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 152/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5007 - accuracy: 0.4580 - val_loss: 1.4900 - val_accuracy: 0.4586\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 153/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.5008 - accuracy: 0.4548 - val_loss: 1.4850 - val_accuracy: 0.4552\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 154/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4991 - accuracy: 0.4569 - val_loss: 1.4832 - val_accuracy: 0.4636\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 155/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.4957 - accuracy: 0.4550 - val_loss: 1.4862 - val_accuracy: 0.4570\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 156/160\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.4962 - accuracy: 0.4573 - val_loss: 1.4899 - val_accuracy: 0.4538\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 157/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4932 - accuracy: 0.4596 - val_loss: 1.4819 - val_accuracy: 0.4638\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 158/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4976 - accuracy: 0.4574 - val_loss: 1.4945 - val_accuracy: 0.4526\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 159/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4956 - accuracy: 0.4552 - val_loss: 1.4790 - val_accuracy: 0.4586\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0009999999310821295.\n",
      "Epoch 160/160\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.4931 - accuracy: 0.4586 - val_loss: 1.4856 - val_accuracy: 0.4624\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=160, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "AJ6-JGpE8YAP",
    "outputId": "3118c213-695f-4198-b597-eb094fa3998e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 3ms/step - loss: 1.4819 - accuracy: 0.4619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4819425344467163, 0.461899995803833]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "DyOosBGh8YAR",
    "outputId": "1491c4d1-8044-4e7e-fc38-ea64d5d42d06"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet 14 - Expressive power of BatchNormalization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
