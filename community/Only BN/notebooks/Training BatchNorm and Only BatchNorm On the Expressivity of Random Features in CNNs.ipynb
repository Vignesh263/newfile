{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Training BatchNorm and Only BatchNorm: On the Expressivity of Random Features in CNNs",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tuOe1ymfHZPu",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# Training BatchNorm and Only BatchNorm: On the Expressivity of Random Features in CNNs\n",
        "\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/template/notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/template/notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "The expressive power of BatchNormalization is an under investigated topic. This [paper](https://arxiv.org/abs/2003.00152) from FAIR goes on to investigate the expressivity that comes from the 'beta' and 'gamma' parameters of BatchNormalization with extensive ablation studies and experiments. The particular position of 'gamma' and 'beta' as per-feature coefficient and bias. Batchnorm makes the optimization landscape smoother and also decouples the optimization of the weight magnitude and the direction of gradients. Like a gift that keeps on giving, BatchNorm also performs a novel regularization and explicitly casues the gradients to reach equilibrium. This model and the associated notebook revolves around reproducing the results from the paper. \n",
        "#  \n",
        "Two points to consider: The TensorFlow image translate function from TF Addons did not perform as well as the paper authors have claimed. The same holds for the weight_decay that was used as part of SGDW from TF Addons. Alternately, the use of l2 kernel regularizer also led to training divergence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vNbeBoBWsDfa"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBluBf7GsDfb",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import random\n",
        "print(tf.version.VERSION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gnv2z7-O2clU",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Input\n",
        "from tensorflow.keras.layers import add, AveragePooling2D, Dense, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow_addons.optimizers import SGDW\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.datasets.cifar10 import load_data\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qT8LP0hfsDfi",
        "colab": {}
      },
      "source": [
        "# Import the data\n",
        "(X_train, y_train), (X_test, y_test) = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-SQU5PLsDfn"
      },
      "source": [
        "## Image augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kGSBMQVqsDfo",
        "colab": {}
      },
      "source": [
        "def augment(img, label):\n",
        "    image = tf.cast(img, tf.float32)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.resize_with_pad(image, 36, 36)\n",
        "    image = tf.image.random_crop(image, size=[32, 32, 3])\n",
        "    image = (image / 255.0)\n",
        "    return image, label\n",
        "\n",
        "def normalize(img, label):\n",
        "    image = tf.cast(img, tf.float32)\n",
        "    image = (image / 255.0)\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGoLVR-BgwH1",
        "colab_type": "text"
      },
      "source": [
        "## tf.data Training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vDMpHHfg2c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val = X_train[5000:], X_train[:5000]\n",
        "y_train, y_val = y_train[5000:], y_train[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMMydC5Ag3n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.cache().shuffle(2048).map(augment, AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(128).prefetch(AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "val_dataset = val_dataset.cache().shuffle(2048).map(augment, AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(128).prefetch(AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.cache().shuffle(2048).map(augment, AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(128).prefetch(AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WlmlEVva1lgv"
      },
      "source": [
        "## Defining callbacks to change the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iSzYk0oH10mv",
        "colab": {}
      },
      "source": [
        "# Creating a custom callback to change the learning rate at 80 and 120 epochs.\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch in [80, 120]:\n",
        "        return lr * 0.1\n",
        "    return lr\n",
        "    \n",
        "callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fhrD1MEvsDft"
      },
      "source": [
        "## Only BN ResNet Architecture for CIFAR10\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "-  We use 1x1 to increase the number of channels to create a wider model with minimum increase in trainable parameters.\n",
        "- This [reserach paper](https://arxiv.org/abs/1812.01187) documents improved accuracy with AveragePooling2D in the shortcut connection. This model showed a performance drop and hence was replaced with a 1x1 convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cNdKeWjwsDfz",
        "colab": {}
      },
      "source": [
        "class ResNet:\n",
        "    \n",
        "    def residual_block(data, filters, strides, transition):\n",
        "        shortcut = data\n",
        "\n",
        "        x = Conv2D(filters, 3, strides, padding=\"same\", kernel_initializer=\"he_normal\", \n",
        "                   use_bias=False)(data)\n",
        "        x = BatchNormalization(beta_initializer='zeros', gamma_initializer=RandomNormal(mean=0.0, stddev=1.0))(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        \n",
        "        x = Conv2D(filters, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\", \n",
        "                   use_bias=False)(x)\n",
        "        x = BatchNormalization(beta_initializer='zeros', gamma_initializer=RandomNormal(mean=0.0, stddev=1.0))(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        \n",
        "        if transition:\n",
        "            shortcut = Conv2D(filters, 1, 2, padding=\"valid\", kernel_initializer=\"he_normal\", \n",
        "                              use_bias=False)(shortcut)\n",
        "            shortcut = BatchNormalization(beta_initializer='zeros', \n",
        "                                          gamma_initializer=RandomNormal(mean=0.0, stddev=1.0))(shortcut)\n",
        "\n",
        "        return add([shortcut, x])\n",
        "    \n",
        "    def build(num_blocks=2, filters_block=[16,32,64]):\n",
        "        inputs = Input(shape=(32,32,3))\n",
        "        x = Conv2D(16, 3, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n",
        "        \n",
        "        for i in range(3):\n",
        "            for j in range(num_blocks):\n",
        "                if j==0:\n",
        "                    transition = True\n",
        "                    strides = 2\n",
        "                else:\n",
        "                    transition = False\n",
        "                    strides = 1\n",
        "                    \n",
        "                x = ResNet.residual_block(x, filters_block[i], strides, transition)\n",
        "                \n",
        "        avg_pool = AveragePooling2D(3)(x)\n",
        "        x = Dense(10, use_bias=False, kernel_initializer='he_normal')(avg_pool)\n",
        "        x = Flatten()(x)        \n",
        "        outputs = Activation(\"softmax\")(x)\n",
        "                \n",
        "        return tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d31xznchs1n_",
        "colab_type": "text"
      },
      "source": [
        "## Build the model\n",
        "---\n",
        "The parameter count matches the exact count in the paper. Please use the appropriate value for `num_blocks` to train the required ResNet architecture.\n",
        "- ResNet-14 (use `num_blocks=2`)\n",
        "- ResNet-32 (use `num_blocks=5`)\n",
        "- ResNet-56 (use `num_blocks=9`)\n",
        "- ResNet-110 (use `num_blocks=18`)\n",
        "- ResNet-218 (use `num_blocks=36`)\n",
        "- ResNet-434 (use `num_blocks=72`)\n",
        "- ResNet-866 (use `num_blocks=144`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oKVErJnbsDf5",
        "colab": {}
      },
      "source": [
        "model = ResNet.build(num_blocks=2) # This trains a ResNet-14 model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQTxLqW5h8Sd",
        "colab_type": "text"
      },
      "source": [
        "## Set only batchnorm layers to be trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxys7TpriDAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_conv = 0\n",
        "for layer in model.layers:\n",
        "    if not isinstance(layer, BatchNormalization):\n",
        "        if hasattr(layer, 'trainable'):\n",
        "            layer.trainable = False\n",
        "    if isinstance(layer, Conv2D):\n",
        "        count_conv += 1\n",
        "print(f'Total Number of Conv layers: {count_conv - 2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UD15cK08sDf-",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H9GdQWfCsDgE"
      },
      "source": [
        "## Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kib6v8vhsDgI",
        "colab": {}
      },
      "source": [
        "step = tf.Variable(0, trainable=False)\n",
        "schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    [28125, 42185], [1e-0, 1e-1, 1e-2])\n",
        "\n",
        "wd = lambda: 1e-4 * schedule(step)\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=0.1, momentum=0.9), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5rwwKT2EsDgO"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztTE-5SJ83jy",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=160, batch_size=128, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSR7LpbuiR0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFYfBCXkiVXb",
        "colab_type": "text"
      },
      "source": [
        "## Visualize the training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouEPIjqGiUps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mwZkcCnEsDgU"
      },
      "source": [
        "## Reference\n",
        "\n",
        "---\n",
        "\n",
        "```\n",
        "@misc{frankle2020training,\n",
        "    title={Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs},\n",
        "    author={Jonathan Frankle and David J. Schwab and Ari S. Morcos},\n",
        "    year={2020},\n",
        "    eprint={2003.00152},\n",
        "    archivePrefix={arXiv},\n",
        "    primaryClass={cs.LG}\n",
        "}\n",
        "```"
      ]
    }
  ]
}